\section{Introduction}
\label{sec:intro}

As a U.S. Department of Energy (DOE) leadership computing facility, the Oak
Ridge Leadership Facility (OLCF) at the Oak Ridge National Laboratory (ORNL)
provides world-leading computing capabilities to scientists and engineers for
accelerating major scientific discoveries and engineering breakthroughs for
humanity, in partnership with the computational science community. The OLCF has
deployed different HPC architectures that are 10 to 100 times more powerful
than typical research computing systems. The OLCF currently hosts Titan, the
world's second fastest supercomputer~\cite{titan}, as well as a
number of analysis and visualization platforms. The OLCF is a leader in large-scale
parallel file system development, design, deployment, and continuous operation. In the
last decade, the OLCF has designed and deployed two large-scale center-wide
parallel file systems, namely, the Spider 1 and Spider 2 systems~\cite{spider1,
spider2}.

An understanding of current storage system workload is critical for
architecting next generation systems, and in the design and development of
storage subsystems. A number of characterization studies have been done at the
storage server level~\cite{hpca04:zhang, iiswc08:swaroop}; characterizing disk level
traces~\cite{ sigmetrics09:alma}, and a number studies on enterprise I/O
workloads~\cite{gmach2007workload, hpca04:zhang}. There have been very limited
studies on the characterization of HPC storage system workloads with trace
data collected from the backend storage controllers. However, a number of HPC
workload characterization studies have focused on I/O characterization from a
scientific application perspective by instrumenting application code for I/O
trace collection~\cite{Carns:2011, shan2008characterizing}. Nevertheless,
these studies have proven useful in understanding the application workloads
for architecting storage systems. Our study is unique in a way that we are
using the traces collected at the backend RAID controller to understand
scientific user workloads and for provisioning storage system resources. Our
prior work ~\cite{ spider1-workload} was on the characterization and study of
the workloads on our first center-wide parallel file system, Spider 1. The
lessons learned from studing Spider 1 helped us in the design and deployment of our
next file system, Spider 2. In this paper, we present a comparative study of
the workload characteristics of our storage system as we scaled from Spider 1,
a 240 GB/s 10 Petabyte, to Spider 2, a +1 TB/s, 32 Petabyte storage system
serving over 20,000 filesystem clients at the OLCF.

%In this paper, we characterize the I/O workload of Spider 2, a center-wide
%storage system providing 32 PB of capacity and an aggregate I/O bandwidth in
%excess of 1 TB/s to over 20,000 file system clients on Titan and the other OLCF
%compute, analysis, and visualization resources. 
%Our work is a follow-up of our previous effort, which analyzed the I/O workload
%on the Spider 1 file system~\cite{spider1-workload}. 

We collected and analyzed file and storage system data from a large-scale
production scientific supercomputing complex. Our data and analysis provides
useful insights on the I/O patterns, in particular I/O bandwidth utilization,
request size distributions, and request latency distribution. Our observation of
drastically increased peak performance overshadowed by the relative low
utilization echoes the design trend of employing a burst-buffer layer. Our
analysis, though not exhaustive, can be beneficial to the
larger community and can be used in building more efficient next-generation
large-scale storage systems for scientific workloads.
