\section{Introduction}
\label{sec:intro}

As a U.S. Department of Energy (DOE) leadership computing facility, the Oak
Ridge Leadership Facility (OLCF) at the Oak Ridge National Laboratory (ORNL)
provides world-leading computing capabilities to scientists and engineers for
accelerating major scientific discoveries and engineering breakthroughs for
humanity, in partnership with the computational science community. The OLCF has
deployed different HPC architectures that are 10 to 100 times more powerful
than typical research computing systems. The OLCF currently hosts Titan,
world's second fastest supercomputer~\cite{titan}, as well as a number of a
number of analysis and visualization platforms.


The Oak Ridge Leadership Computing Facility (OLCF) is a leader in large-scale
parallel file system development, deployment, and continuous operation. In the
last decade, the OLCF has designed and deployed two large-scale center-wide
parallel file systems, namely, Spider 1 and Spider 2 systems~\cite{spider1,
spider2}. 

In this paper, we characterize the I/O workload of Spider 2, a large-scale
Lustre parallel file system~\cite{spider2} deployed at the OLCF. Spider 2 hosts
data for scientific jobs running on the world's fastest supercomputer, Titan,
and a host of other compute infrastructures at the Oak Ridge Leadership
Computing Facility (OLCF) at Oak Ridge National Laboratory (ORNL). As a
center-wide storage system, Spider 2 is a 32 PB storage system capable of
providing an aggregate I/O bandwidth in excess of 1 TB/s close to 20,000
file system clients. 

Our work is a follow-up of our previous effort, which analyzed the I/O workload
on the Spider 1 file system~\cite{spider1-workload}.  Our analysis of
scientific workloads has <how many?> main contributions: 

\begin{itemize}


\item We collected and analyzed file and storage system data from a large-scale
production scientific supercomputing complex. Our data and analysis provides
useful insight on the I/O patterns, in particular I/O bandwidth, request size
distribution requirements. Our observations and analysis can be used in
building more efficient next-generation large-scale storage systems for
scientific workloads.  

\item Contribution 2

\item Any others? 

\end{itemize}
